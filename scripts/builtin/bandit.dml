#-------------------------------------------------------------
#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

m_bandit = function(Matrix[Double] X_train, Matrix[Double] Y_train, Matrix[Double] mask, Matrix[Double] MLhp,
  Frame[Unknown] schema, Frame[Unknown] lp, Frame[Unknown] primitives, Frame[Unknown] param,  Integer k = 3,
  Double testAccuracy = 0.8, Boolean isWeighted, Integer R=50, Integer cv=3, Boolean verbose = TRUE)
  return (Frame[Unknown] bestPipeline, Matrix[Double] bestHyperparams,  Matrix[Double] bestAccuracy) 
{
  print("null in data "+sum(is.na(X_train)))
  bestPipeline = frame("", rows=1, cols=1)
  bestHyperparams = as.matrix(0)
  bestAccuracy = as.matrix(0)
  # hparam = list()

  # pipeline = list()
  
  # initialize bandit variables
  # variable names follow publication where algorithm is introduced
  eta = 2  # the halving ratio is fixed to 2
  s_max = floor(log(R,eta));
  B = (s_max + 1) * R;
  
    # initialize output variables
  hparam = matrix(0, rows=k*(s_max+1), cols=55)
  pipeline = frame("|", rows=k*(s_max+1), cols=ncol(lp)+1)
  start=0; end=0;
  for(s in s_max:0, check = 0) {
    
    # bracket_hp = list()
    bracket_hp = matrix(0, rows=k*(s+1), cols=55)
    bracket_pipel = matrix(0, rows=k*(s+1), cols=3)
    
    n = ceil(floor(B/R/(s+1)) * eta^s);
    r = R * eta^(-s);
    configurations = get_physical_configurations(lp, n, primitives)
    # append configuration ids
    id = seq(1, nrow(configurations))
    configurations = cbind(as.frame(id), configurations)
    lookup = configurations
    
    if(verbose) 
      print("n "+ n +"\n R "+ R +"\n s_max "+ s_max +"\n B "+ B +"\n n "+ n +"\n r "+ r)
    
    for( i in 0:s,check=0 ) {
      # successive halving    
      n_i = as.integer(floor(n * eta^(-i)));
      r_i = as.integer(floor(r * eta^i));
      
      if(verbose) {
        print("no of configurations ---------"+n_i)
        print("no of resources --------------"+r_i)
        print("iteration  ---------------------"+i)
      }
      configurations = configurations[1:n_i, ]
      print("conf successive halving")
      print(toString(configurations))
      
      [outPip,outHp] = run_with_hyperparam(configurations, r_i, X_train, Y_train, mask, 
        MLhp, schema, param, isWeighted, testAccuracy, cv,  verbose)
      # sort the pipelines by order of accuracy decreasing
      # a = frameSort(a)
      a = order(target = outPip, by = 1, decreasing=TRUE, index.return=FALSE)
      b = order(target = outHp, by = 1, decreasing=TRUE, index.return=FALSE)
      rowIndex = ifelse(nrow(a) > k, k, nrow(a))
      # maintain the brackets results
      # 1:5, 6:10, 11:15
      # k*s+1, k*(s+1), s = 0, 1: 5, s=1,  6:10, s=2 11:15 
      # bracket_pipel = append(bracket_pipel, a[1:rowIndex,])
      bracket_pipel[(k*i)+1:k*(1+i), ] =  a[1:rowIndex,]
      bracket_hp[(k*i)+1:k*(1+i), 1:ncol(b)] =  b[1:rowIndex,]
      # sort the configurations fro successive halving
      avergae_perf =  getMaxPerConf(outPip, r_i)     #as.frame(aggregate(target=a[, 1], groups=a[, 2], fn="mean"))
      configurations = frameSort(cbind(avergae_perf, configurations))
      configurations = configurations[, 2:ncol(configurations)]

    }
    # keep the best k results for each bracket
    [bracket_bestPipeline, bracket_bestHyperparams] = extractBracketWinners(bracket_pipel, bracket_hp, k, lookup)
    
    print("after "+i+" bracket ")
    print(toString(bracket_bestPipeline))
    print("------------------")
    print(toString(bracket_bestHyperparams))  

    while(FALSE){}
    start = end + 1
    end = end + nrow(bracket_bestPipeline)
    pipeline[(k*s)+1:k*(1+s), ] = bracket_bestPipeline
    hparam[(k*s)+1:k*(1+s), 1:ncol(bracket_bestHyperparams)] = bracket_bestHyperparams
  }
  
  print("after all brackets ")
  while(FALSE){}
  print(toString(pipeline))
  print("------------------")
  print(toString(hparam))
  while(FALSE){}
  # extract best top k from all iterations
  [bestPipeline, bestHyperparams] = extractTopK(pipeline, hparam, testAccuracy, k)

  bestAccuracy = as.matrix(bestPipeline[,1])
  bestPipeline = bestPipeline[,2:ncol(bestPipeline)]
  bestHyperparams = bestHyperparams[,2:ncol(bestHyperparams)]
  
  if(verbose) {
    print("best pipeline"+ toString(bestPipeline))
    print("best hyper-parameters \n"+ toString(bestHyperparams))
    print("best accuracy \n"+ toString(bestAccuracy))
    print("dirty accuracy "+testAccuracy)
  }
}


# this method will extract the physical pipelines for a given logical pipelines
#TODO resources and configuration
get_physical_configurations = function(Frame[String] logical, Scalar[int] numConfigs, 
  Frame[Unknown] primitives)
  return(Frame[String] physical)
{

  # load the primitives
  physical = as.frame("NaN")
  outliers = primitives[,1]
  mvi = primitives[,2]
  noise = primitives[,3]
  ci = primitives[,4]
  dim = primitives[,5]
  dummy = primitives[,6]
  scale = primitives[,7]

 
  operator = as.frame(matrix(0,nrow(outliers),1)) #combine all logical primitives
  for(j in 1:ncol(logical))
  {
    # extract the physical primitives
    if(as.scalar(logical[1,j]) == "OTLR")
      operator = cbind(operator, outliers);
    else if(as.scalar(logical[1,j]) == "MVI")
      operator = cbind(operator, mvi);
    else if(as.scalar(logical[1,j]) == "NR")
      operator = cbind(operator, noise);  
    else if(as.scalar(logical[1,j]) == "CI")
      operator = cbind(operator, ci);
    else if(as.scalar(logical[1,j]) == "DIM")
      operator = cbind(operator, dim);
    else if(as.scalar(logical[1,j]) == "DUMMY")
      operator = cbind(operator, dummy);  
    else if(as.scalar(logical[1,j]) == "SCALE")
      operator = cbind(operator, scale);  
  }
  opt = operator[,2:ncol(operator)] 

  idx = seq(1, ncol(opt))
  # get the indexes of columns for recode transformation
  index = vectorToCsv(idx)
  # recode logical pipelines for easy handling
  jspecR = "{ids:true, recode:["+index+"]}";
  [X, M] = transformencode(target=opt, spec=jspecR);
  X = replace(target= X, pattern = NaN, replacement = 0)
  
  paramLens = matrix(0, ncol(logical), 1);
  for( j in 1:ncol(logical)) {
    vect = removeEmpty(target = X[,j], margin = "rows");
    paramLens[j,1] = nrow(vect);
  }
   paramVals = matrix(0, ncol(logical), max(paramLens));
   for( j in 1:ncol(logical) ) {
    vect = removeEmpty(target = X[,j], margin = "rows");
    paramVals[j,1:nrow(vect)] = t(vect);
  }
  cumLens = rev(cumprod(rev(paramLens))/rev(paramLens));
  # materialize hyper-parameter combinations 
  HP = matrix(0, numConfigs, ncol(logical));
  pip = sample(numConfigs,numConfigs)
  for( i in 1:nrow(HP) ) {
    for( j in 1:ncol(logical) ) {
      HP[i,j] = paramVals[j,as.scalar((as.scalar(pip[i,1])/cumLens[j,1])%%paramLens[j,1]+1)];
    }
  }
  
  physical = transformdecode(target=HP, spec=jspecR, meta=M);
  print("physical pipeline "+toString(physical))
}

# this method will call the execute pipelines with their hyper-parameters
run_with_hyperparam = function(Frame[Unknown] ph_pip, Integer r_i, Matrix[Double] X, Matrix[Double] Y,
  Matrix[Double] mask, Matrix[Double] MLhp, Frame[Unknown] schema, Frame[Unknown] param, Boolean isWeighted,
  Double testAccuracy, Integer cv=3, Boolean verbose)                    
  return (Matrix[Double] output_operator, Matrix[Double] output_hyperparam)
{

  output_hp = matrix(0, nrow(ph_pip)*r_i, 50)
  output_accuracy = matrix(0, nrow(ph_pip)*r_i, 1)
  output_pipelines = matrix(0, nrow(ph_pip)*r_i, 2)

  # rows in validation set
  clone_X = X
  clone_Y = Y
  index = 1
  id = as.matrix(ph_pip[, 1])
  ph_pip = ph_pip[, 2:ncol(ph_pip)]
  for(i in 1:nrow(ph_pip))
  {
    # execute configurations with r resources
    for(r in 1:r_i)
    {
      hp = getHyperparam(ph_pip[i,], param)      
      [X, Y] = executePipeline(ph_pip[i], X, Y, mask, schema, hp, FALSE)

      # [X_val, Y_val] = executePipeline(ph_pip[i], X_val, Y_val, mask, schema, hp, FALSE)
      # accuracy = fclassify(X, Y, X_val, Y_val, mask, isWeighted)
      accuracy = fclassify(X, Y, mask, MLhp, testAccuracy, isWeighted, cv)

      hp_vec = listToVector(hp, FALSE)
      print("hp stored ----------\n"+toString(hp_vec)
      )
      output_accuracy[index, 1] = accuracy
      output_hp[index, 1:ncol(hp_vec)] = hp_vec
      output_pipelines[index, ] = cbind(as.matrix(i), id[i,1])
      X = clone_X
      Y = clone_Y
      while(FALSE){}
      index = index + 1
    }
    X = clone_X
    Y = clone_Y
  }
  output_hyperparam = cbind(output_accuracy, output_hp)
  output_operator = cbind(output_accuracy, output_pipelines)
}

# extract the hyper-parameters for pipelines
getHyperparam = function(Frame[Unknown] pipeline, Frame[Unknown]  hpList)
  return (List[Unknown] paramList)
{
  # load the hyper-parameters values
  paramList = list()
  for(i in 1:ncol(pipeline)) {
    op = as.scalar(pipeline[1,i])
    hasParam = map(hpList[,1], "x->x.contains(\""+op+"\")")
    m_hasParam = matrix(0, nrow(hasParam), 1)

    # convert the boolean vector to 0/1 matrix representation
    for(h in 1:nrow(hasParam))
      m_hasParam[h] = ifelse(as.scalar(hasParam[h,1]) == "true",1,0)
    # compute the relevant index 
    index = m_hasParam * seq(1, nrow(m_hasParam))
    index = as.scalar(removeEmpty(target = index, margin = "rows"))
    no_of_param = as.integer(as.scalar(hpList[index, 2]))

    # extract hasY and verbose flags
    attachMask = as.matrix(hpList[index, 3])
    attachY = as.matrix(hpList[index, 4])
    isVerbose = as.matrix(hpList[index, 5])
    dataFlag = as.matrix(hpList[index, 6])
    
    if(no_of_param > 0) {
      start = 7
      t = 7
      OpParam = matrix(0, 1, no_of_param)
      for(j in 1:no_of_param) {
        type = as.scalar(hpList[index, t])
        paramValIndex = (no_of_param) + start
        minVal =  as.scalar(hpList[index, paramValIndex])
        maxVal = as.scalar(hpList[index, paramValIndex + 1])
        [minVal, maxVal] = verifyHp(i, pipeline, minVal, maxVal, j)
        if(type == "FP") {
          val = as.scalar(rand(rows=1, cols=1, min=minVal,
                          max=maxVal, pdf="uniform"));
          OpParam[1, j] = val
        }
        else if(type == "INT") {
          # val = ifelse(minVal == maxVal , minVal, as.scalar(sample(maxVal, 1)));
          val = round(as.scalar(rand(rows=1, cols=1, min=minVal, 
                                max=maxVal, pdf="uniform")));
          OpParam[1, j] = val
        }
        else if(type == "BOOL") {
          s = as.scalar(sample(2,1))
          b = as.integer(s-1)
          OpParam[1, j] = b
        }
        else {
          # TODO handle string set something like {,,}
          print("invalid data type")
        }
        start = start + 2
        t = t + 1
      }
      OpParam = cbind(OpParam, attachMask, attachY, isVerbose, dataFlag)
    }
    else {
      OpParam = cbind(attachMask, attachY)
      OpParam = cbind(OpParam, isVerbose, dataFlag)
    }
    while(FALSE){}
    paramList = append(paramList, OpParam)
  }
}


listToVector = function(List[Unknown] hp, Boolean verbose)
return (Matrix[Double] hp_vec)
{
  hp_vec = matrix(0,1,1)
  len = length(hp)
  for(k in 1:len) {
    mat = as.matrix(hp[k])
    hpy = cbind(as.matrix(ncol(mat)), mat)
    hp_vec = cbind(hp_vec, hpy)
  }
  hp_vec = hp_vec[1, 2:ncol(hp_vec)]
}


fclassify = function(Matrix[Double] X, Matrix[Double] Y, Matrix[Double] mask, Matrix[Double] MLhp,
  Double testAccuracy, Boolean isWeighted, Integer cv=3)
  return (Double accuracy)
{
 
  if(max(Y) == min(Y)) {
    print("Y contains only one class")
    accuracy = as.double(0)
  }
  else { 
    print("STARTING "+cv+" CROSS VALIDATIONS")
    # do the k = 3 cross validations
    accuracyMatrix = crossV(X, Y, cv, mask, MLhp, isWeighted, testAccuracy)
    accuracyMatrix = removeEmpty(target=accuracyMatrix, margin="rows")
    acc = colMeans(accuracyMatrix)
    accuracy = as.scalar(acc[1,1])
    print("validation accuracy "+accuracy)
  }
}

# crossV = function(Matrix[double] X, Matrix[double] y, Integer k, Matrix[Double] mask,
  # Matrix[Double] MLhp, Boolean isWeighted, Double testAccuracy) 
# return (Matrix[Double] accuracyMatrix)
# {

  # accuracyMatrix = matrix(0, k, 2)

  # trainL = list()
  # testL = list()
  # for(i in 1:k)
  # {
    # [trainX, trainy, testX, testy] = splitBalanced(X, y, 0.8, FALSE)
    # trainL = append(trainL, cbind(trainX, trainy))
    # testL = append(testL, cbind(testX, testy))
    # X=rbind(testX,trainX)
    # y = rbind(testy,trainy)
    
  # }
  # for(i in 1:k)
  # {
      # trainX = as.matrix(trainL[i])
      # testX = as.matrix(testL[i])
      # trainy = trainX[,ncol(trainX)]
      # testy = testX[,ncol(testX)]
      # trainX =trainX[, 1:ncol(trainX)-1]
      # testX =testX[, 1:ncol(testX)-1]
      # beta = multiLogReg(X=trainX, Y=trainy, icpt=1, reg=as.scalar(MLhp[1,1]), tol= 1e-9, 
      # maxi=as.scalar(MLhp[1,2]), maxii= 50, verbose=FALSE);
      # [prob, yhat, a] = multiLogRegPredict(testX, beta, testy, FALSE)
      # accuracy = getAccuracy(testy, yhat, isWeighted)
      # splitAccuracy = accuracy
      # accuracyMatrix[i, 1] = accuracy
  # }

# }
crossV = function(Matrix[double] X, Matrix[double] y, Integer k, Matrix[Double] mask,
  Matrix[Double] MLhp, Boolean isWeighted, Double testAccuracy) 
return (Matrix[Double] accuracyMatrix)
{

  accuracyMatrix = matrix(0, k, 2)

  dataList = list()
  testL = list()
  data = order(target = cbind(y, X),  by = 1, decreasing=FALSE, index.return=FALSE)
  classes = table(data[, 1], 1)
  ins_per_fold = classes/k
  start_fold = matrix(1, rows=nrow(ins_per_fold), cols=1)
  fold_idxes = cbind(start_fold, ins_per_fold)

  start_i = 0; end_i = 0; idx_fold = 1;;
  for(i in 1:k)
  {
    fold_i = matrix(0, 0, ncol(data))
    start=0; end=0; 
    for(j in 1:nrow(classes))
    {
      idx = as.scalar(classes[j, 1])
      start = end + 1;
      end = end + idx
      class_j =  data[start:end, ]


      start_i = as.scalar(fold_idxes[j, 1]);
      end_i = as.scalar(fold_idxes[j, 2])

      fold_i = rbind(fold_i, class_j[start_i:end_i, ])
    }

    dataList = append(dataList, fold_i)
    fold_idxes[, 1] = fold_idxes[, 2] + 1
    fold_idxes[, 2] += ins_per_fold
    while(FALSE){}
  }

  for(i in seq(1,k))
  {
      [trainList, hold_out] = remove(dataList, i)
      trainset = rbind(trainList)
      testset = as.matrix(hold_out)
      trainX = trainset[, 2:ncol(trainset)]
      trainy = trainset[, 1]
      testX = testset[, 2:ncol(testset)]
      testy = testset[, 1]
      beta = multiLogReg(X=trainX, Y=trainy, icpt=1, reg=as.scalar(MLhp[1,1]), tol= 1e-9, 
      maxi=as.scalar(MLhp[1,2]), maxii= 50, verbose=FALSE);
      [prob, yhat, a] = multiLogRegPredict(testX, beta, testy, FALSE)
      accuracy = getAccuracy(testy, yhat, isWeighted)
      accuracyMatrix[i, 1] = accuracy
  }

}



# extract the top k pipelines
extractTopK = function(Frame[Unknown] pipeline, Matrix[Double] hyperparam, 
  Double testAccuracy, Integer k)
  return (Frame[Unknown] bestPipeline, Matrix[Double] bestHyperparams)
{

  # sort results
  hyperparam = order(target = hyperparam, by = 1, decreasing=TRUE, index.return=FALSE)
  pipeline = frameSort(pipeline)
  
  if(as.scalar(pipeline[1,1]) < testAccuracy)
  {
    bestPipeline = pipeline[1,]
    bestHyperparams = hyperparam[1,]
  }
  else {
    # remove the row with accuracy less than test accuracy 
    mask = (hyperparam[, 1] < testAccuracy) == 0
    hyperparam = removeEmpty(target = hyperparam, margin = "rows", select = mask)
    rowIndex = ifelse(nrow(hyperparam) > k, k, nrow(hyperparam))
    # select the top k
    bestPipeline = pipeline[1:rowIndex,]
    bestHyperparams = hyperparam[1:rowIndex,]
  }
}

# extract the top k pipelines
extractBracketWinners = function(Matrix[Double] pipeline, Matrix[Double] hyperparam, 
  Integer k, Frame[Unknown] conf)
  return (Frame[Unknown] bestPipeline, Matrix[Double] bestHyperparams)
{

  # bestPipeline = frameSort(bestPipeline)
  hyperparam = order(target = hyperparam, by = 1, decreasing=TRUE, index.return=FALSE)
  pipeline = order(target = pipeline, by = 1, decreasing=TRUE, index.return=FALSE)
  
  rowIndex = ifelse(nrow(pipeline) > k, k, nrow(pipeline))

  pipeline = pipeline[1:rowIndex,]
  bestHyperparams = hyperparam[1:rowIndex,]
  bestPipeline = frame(data="|", rows=nrow(pipeline), cols=ncol(conf)-1)
  for(i in 1: nrow(pipeline), check=0)
  {
    index = as.scalar(pipeline[i, 3])
    bestPipeline[i, ] = conf[index, 2:ncol(conf)]
  }
  bestPipeline = cbind(as.frame(pipeline[, 1]),  bestPipeline)
  
}

# remove empty wrapper for frames
# frameRmEmpty = function(Frame[Unknown] frameblock, Matrix[Double] selectMatrix)
# return (Frame[Unknown] frameblock)
# {
  # idx = seq(1, ncol(frameblock))
  # # get the indexes of columns for recode transformation
  # index = vectorToCsv(idx)
  # # recode logical pipelines for easy handling
  # jspecR = "{ids:true, recode:["+index+"]}";
  # [X, M] = transformencode(target=frameblock, spec=jspecR);
  # X = removeEmpty(target = X, margin = "rows", select = selectMatrix)
  # frameblock = transformdecode(target = X, spec = jspecR, meta = M)
# }


# smote wrapper for doing relative over-sampling
SMOTE  = function(Matrix[Double] X, Matrix[Double] Y,  Boolean verbose)
return (Matrix[Double] XY)
{

  XY = order(target = cbind(Y, X),  by = 1, decreasing=FALSE, index.return=FALSE)
  # get the class count 
  classes = table(Y, 1)
  print("before smote")
  print(toString(classes))
  start_class = 1
  end_class = 0
  k = table(Y, 1)
  getMax = max(k)
  maxKIndex = as.scalar(rowIndexMax(t(k)))
  outSet = matrix(0, 0, ncol(XY))
 
  for(i in 1: nrow(k)) {
    end_class = end_class + as.scalar(classes[i])
    class_t = XY[start_class:end_class, ]
    remainingRatio = (round(getMax/nrow(class_t)) - 1) * 100
    if((i != maxKIndex)) {
      # TODO implement SMOTE-NC for categorical data oversampling
      synthesized = smote(class_t, remainingRatio, 1, FALSE)
      outSet = rbind(outSet, synthesized)
      if(verbose) {
        print("max value: "+getMax)
        print("values of i: "+i)
        print("remaining ratio: "+remainingRatio)
      }
    }
    start_class = end_class + 1
  }
  
  XY = rbind(XY, synthesized)
  Y = XY[, 1]
  X = XY[, 2:ncol(XY)]
  XY = cbind(X,Y)
  classes = table(Y, 1)
  print("after smote")
  print(toString(classes))
}

# constraints over hyper parameters
verifyHp = function(Integer index, Frame[Unknown] pip, Double minVal, Double maxVal, Integer paraNo)
return (Double minVal, Double maxVal) {
  op = as.scalar(pip[1,index])
  # 1. if next op is pca then current op should not leave NaNs in data
  # 2. if next op is mice then current op should not replace NaNs with zeros
  
  if((op == "outlierBySd" | op == "outlierByIQR") & index < ncol(pip) & paraNo == 2)
  {
    nextOp = as.scalar(pip[1, index + 1])
    if(nextOp == "pca" | nextOp == "abstain" | nextOp == "SMOTE")
    {
      maxVal = 1.0
    }
    if(nextOp == "mice")
    {
      minVal = 2.0
    }
  }
  # print("now min and max val ")
  # print(minVal+" "+maxVal)
  
}

# smote_nc = function(Matrix[Double] X, Integer s = 200, Matrix[Double] mask, Integer k = 1, Boolean verbose = FALSE) 
# return (Matrix[Double] Y) {

  # if(s < 100 | (s%%100) != 0)
  # {
    # print("the number of samples should be an integral multiple of 100. Setting s = 100")
    # s = 100
  # }
  
  # if(k < 1) {
    # print("k should not be less than 1. Setting k value to default k = 1.")
    # k = 1
  # }
  
  # # matrix to keep the index of KNN for each minority sample
  # knn_index = matrix(0,k,nrow(X))
  # # find nearest neighbour
  # for(i in 1:nrow(X))
  # {
    # knn = nn(X, X[i, ], k)
    # knn_index[, i] = knn
  # }
  
  # # number of synthetic samples from each minority class sample
  # iter = 0
  # iterLim = (s/100)
  # # matrix to store synthetic samples
  # synthetic_samples = matrix(0, iterLim*ncol(knn_index), ncol(X))
  
  # # shuffle the nn indexes
  # #rand_index =  ifelse(k < iterLim, sample(k, iterLim, TRUE, 42), sample(k, iterLim, 42))
  # if (k < iterLim)
    # rand_index = sample(k, iterLim, TRUE, 42);
  # else
    # rand_index = sample(k, iterLim, 42);

  # while(iter < iterLim)
  # {
    # # pick the random NN
    # knn_sample = knn_index[as.scalar(rand_index[iter+1]),] 
    # # generate sample    
    # for(i in 1:ncol(knn_index))
    # {
      # index = as.scalar(knn_sample[1,i])
      # X_diff = X[index,] - X[i, ]
      # gap = as.scalar(Rand(rows=1, cols=1, min=0, max=1, seed = 42))
      # X_sys = X[i, ] + (gap*X_diff)
      # synthetic_samples[iter*ncol(knn_index)+i,] = X_sys;
    # }
    # iter = iter + 1
  # }

  # Y = synthetic_samples
  
  # if(verbose)
    # print(nrow(Y)+ " synthesized samples generated.")

# }
  


# nn = function(Matrix[Double] X, Matrix[Double] instance, Integer k )
# return (Matrix[Double] knn_)
# {
  # if(nrow(X) < k)
    # stop("can not pick "+k+" nearest neighbours from "+nrow(X)+" total instances")

  # # compute the euclidean distance
  # diff = X - instance
  # square_diff = diff^2
  # distance = sqrt(rowSums(square_diff))
  # sort_dist = order(target = distance, by = 1, decreasing= FALSE, index.return =  TRUE)
  # knn_ = sort_dist[2:k+1,]
# }

downSample = function(Matrix[Double] X, matrix[Double] Y, Integer class)
return (Matrix[Double] XY)
{
  # find the class distribution
  classes = table(Y, 1)
  XY = order(target = cbind(X,Y), by = ncol(X), decreasing = FALSE, index.return = FALSE)
  # take minimum class out
  minRecords = min(classes)
  start_class = 1
  out_s = 1 
  out_e = 0
  end_class = 0

  out = matrix(0, minRecords * nrow(classes), ncol(XY))

  for(i in 1:nrow(classes))
  {
    end_class = end_class + as.scalar(classes[i])
    class_t = XY[start_class:end_class, ]
    out_e = out_e + i * minRecords
    out[out_s:out_e, ] = class_t[1:minRecords, ] 
    out_s = out_e + 1
    start_class = end_class + 1
  }

}

fillDefault = function(Matrix[Double] X)
return(Matrix[Double] X){
  defaullt = round(colMaxs(X) - colMins(X))
  Mask = is.na(X)
  X = replace(target=X, pattern=NaN, replacement=1)
  Mask = Mask * defaullt
  X = X + Mask
  # print("defaullt imputed \n"+toString(X, rows=5))
}


dummycoding = function(Matrix[Double] X, Matrix[Double] mask)
return (Matrix[Double] dX_train) {

  Xtrain = replace(target = X, pattern = NaN, replacement=1)
  
  train_max_values = colMaxs(Xtrain)
  dX_train = matrix(0,nrow(Xtrain),0)
  # one-hot-encode the categorical features
  for(i in 1:ncol(mask))
  {
    if(as.scalar(mask[1, i]) == 1)
    {
      max_value = max(train_max_values[1, i])
      encoded = toOneHot(Xtrain[, i], max_value)
      dX_train = cbind(dX_train, encoded)
    }
    else {
      dX_train = cbind(dX_train, Xtrain[, i])
    }

  }
}

getMaxPerConf = function(Matrix[Double] pipelines, Double items)
return (Frame[Unknown] maxperconf)
{

  start = 0
  end = 0
  maxperconf = frame(0, rows=max(pipelines[, 2]), cols=1)
  for(i in 1:nrow(maxperconf))
  {
    start = end + 1
    end = end + items
    sub = max(pipelines[start:end, 1])
    # maxperconf[i, 1] = i
    maxperconf[i, 1] = sub
  }
}
